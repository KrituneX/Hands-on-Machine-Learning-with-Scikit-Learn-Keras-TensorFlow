{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KrituneX/Hands-on-Machine-Learning-with-Scikit-Learn-Keras-TensorFlow/blob/main/Chapter_13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srwD98s48N9r"
      },
      "source": [
        "# Ringkasan Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
        "## Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow (2nd Ed.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "am4fGxOi8N9t"
      },
      "source": [
        "## 1. TensorFlow Data API: Teori Dasar\n",
        "\n",
        "### Arsitektur Pipeline Data\n",
        "$$ \\text{DataSource} \\rightarrow \\text{Transformations} \\rightarrow \\text{Consumer} $$\n",
        "\n",
        "TensorFlow Data API menggunakan pendekatan lazy evaluation:\n",
        "$$ \\text{Data Pipeline} = \\{f_1 \\circ f_2 \\circ ... \\circ f_n\\}(\\text{Dataset}) $$\n",
        "\n",
        "**Komponen Utama**:\n",
        "1. `Dataset`: Representasi abstrak dari aliran data\n",
        "2. `Transformation`: Operasi pada dataset (map, filter, batch, dll.)\n",
        "3. `Iterator`: Mekanisme untuk mengkonsumsi data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gk-cg7kJ8N9t"
      },
      "source": [
        "## 2. Operasi Dasar dengan TF Data API\n",
        "\n",
        "### Transformasi Chaining\n",
        "Rumus komposisi transformasi:\n",
        "$$ \\text{Dataset}' = \\text{Dataset}.\\text{map}(f).\\text{batch}(n).\\text{prefetch}(k) $$\n",
        "\n",
        "**Contoh Implementasi**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqpg5SjP8N9t"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Membuat pipeline data\n",
        "dataset = tf.data.Dataset.range(100)\n",
        "dataset = dataset.map(lambda x: x**2)  # Transformasi 1\n",
        "dataset = dataset.batch(16)            # Transformasi 2\n",
        "dataset = dataset.prefetch(1)          # Optimasi performa\n",
        "\n",
        "for batch in dataset.take(3):\n",
        "    print(\"Batch:\", batch.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYWJSZIt8N9u"
      },
      "source": [
        "## 3. Shuffling dan Windowing\n",
        "\n",
        "### Teori Shuffling\n",
        "Rumus buffer shuffling:\n",
        "$$ \\text{Shuffle}(D, b) = \\text{random_sample}(D, b) $$\n",
        "dimana $b$ adalah ukuran buffer\n",
        "\n",
        "### Window Transformation\n",
        "Untuk data time series:\n",
        "$$ \\text{Window}(D, w, s) = \\{D[i:i+w] \\text{ for } i \\in 0,s,2s,...\\} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Q3bQjqX8N9u"
      },
      "outputs": [],
      "source": [
        "# Contoh shuffling dan windowing\n",
        "dataset = tf.data.Dataset.range(100)\n",
        "dataset = dataset.shuffle(buffer_size=50)\n",
        "dataset = dataset.window(size=5, shift=1, drop_remainder=True)\n",
        "\n",
        "for window in dataset.take(3):\n",
        "    print([item.numpy() for item in window])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ud0ME_S48N9u"
      },
      "source": [
        "## 4. Preprocessing Data\n",
        "\n",
        "### Normalisasi\n",
        "$$ z = \\frac{x - \\mu}{\\sigma} $$\n",
        "\n",
        "### One-Hot Encoding\n",
        "$$ \\text{encode}(x) = [\\mathbb{I}(x=k)]_{k=1}^K $$\n",
        "\n",
        "**Pipeline Lengkap**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qupEJ-1m8N9u"
      },
      "outputs": [],
      "source": [
        "def preprocess(features, label):\n",
        "    # Normalisasi\n",
        "    features = (features - tf.reduce_mean(features)) / tf.math.reduce_std(features)\n",
        "\n",
        "    # One-hot encoding\n",
        "    label = tf.one_hot(label, depth=10)\n",
        "\n",
        "    return features, label\n",
        "\n",
        "# Contoh dataset MNIST\n",
        "(X_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
        "dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "dataset = dataset.map(preprocess).batch(32).prefetch(1)\n",
        "\n",
        "for X_batch, y_batch in dataset.take(1):\n",
        "    print(\"Batch shape:\", X_batch.shape, y_batch.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEGjegD58N9u"
      },
      "source": [
        "## 5. TFRecord Format\n",
        "\n",
        "### Struktur TFRecord\n",
        "$$ \\text{Example} = \\{ \\text{feature}: \\text{Feature}\\} $$\n",
        "\n",
        "**Feature Types**:\n",
        "1. FloatList\n",
        "2. Int64List\n",
        "3. BytesList\n",
        "\n",
        "**Contoh Pembuatan TFRecord**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvamU5NX8N9u"
      },
      "outputs": [],
      "source": [
        "def write_tfrecord(images, labels, filename):\n",
        "    with tf.io.TFRecordWriter(filename) as writer:\n",
        "        for img, lbl in zip(images, labels):\n",
        "            feature = {\n",
        "                'image': tf.train.Feature(float_list=tf.train.FloatList(value=img.flatten())),\n",
        "                'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[lbl]))\n",
        "            }\n",
        "            example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "            writer.write(example.SerializeToString())\n",
        "\n",
        "# Contoh penggunaan\n",
        "write_tfrecord(X_train[:100], y_train[:100], 'mnist_sample.tfrecord')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24cCv_4A8N9v"
      },
      "source": [
        "## 6. Parallel Data Loading\n",
        "\n",
        "### Interleave Pattern\n",
        "$$ \\text{interleave}(D_1,...,D_n) = \\text{round_robin}(D_1,...,D_n) $$\n",
        "\n",
        "**Contoh Implementasi**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxt18HYz8N9v"
      },
      "outputs": [],
      "source": [
        "files = ['data1.tfrecord', 'data2.tfrecord', 'data3.tfrecord']\n",
        "dataset = tf.data.Dataset.from_tensor_slices(files)\n",
        "dataset = dataset.interleave(\n",
        "    lambda x: tf.data.TFRecordDataset(x),\n",
        "    cycle_length=4,\n",
        "    num_parallel_calls=tf.data.AUTOTUNE\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9XE8cGp8N9v"
      },
      "source": [
        "## 7. Best Practices\n",
        "\n",
        "1. **Pipeline Optimization**:\n",
        "$$ \\text{Throughput} = \\frac{\\text{Batch Size}}{\\text{Step Time}} $$\n",
        "\n",
        "2. **Cache Strategy**:\n",
        "$$ \\text{Dataset} = \\text{Dataset}.\\text{cache}() $$\n",
        "\n",
        "3. **Prefetch Pattern**:\n",
        "$$ \\text{Dataset} = \\text{Dataset}.\\text{prefetch}(\\text{buffer_size}=tf.data.AUTOTUNE) $$"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}