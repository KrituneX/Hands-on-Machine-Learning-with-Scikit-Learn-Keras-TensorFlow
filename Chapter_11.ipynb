{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KrituneX/Hands-on-Machine-Learning-with-Scikit-Learn-Keras-TensorFlow/blob/main/Chapter_11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c155b68",
      "metadata": {
        "id": "8c155b68"
      },
      "source": [
        "## Chapter 11: Melatih Jaringan Neural Dalam (Deep Neural Networks)\n",
        "Ringkasan berdasarkan *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow* oleh AurÃ©lien GÃ©ron."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "412db21a",
      "metadata": {
        "id": "412db21a"
      },
      "source": [
        "### 1. Masalah Vanishing dan Exploding Gradients\n",
        "- Dalam jaringan dalam, gradien bisa menjadi sangat kecil (vanishing) atau sangat besar (exploding).\n",
        "- Ini menyebabkan pelatihan lambat atau tidak stabil.\n",
        "- **Solusi**:\n",
        "  - **Inisialisasi Bobot**: He initialization atau Glorot initialization.\n",
        "  - **Aktivasi Tidak Menyaturasi**: ReLU dan variannya lebih stabil dibanding sigmoid/tanh.\n",
        "  - **Batch Normalization**: Menormalkan aktivasi di setiap mini-batch."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88eab2db",
      "metadata": {
        "id": "88eab2db"
      },
      "source": [
        "### 2. Teknik Optimisasi\n",
        "- **Momentum**: Menggunakan momentum gradien untuk percepatan konvergensi.\n",
        "- **Nesterov Accelerated Gradient (NAG)**: Perbaikan dari momentum klasik.\n",
        "- **AdaGrad**: Menyesuaikan learning rate untuk setiap parameter.\n",
        "- **RMSProp**: Perpaduan momentum dan AdaGrad.\n",
        "- **Adam**: Kombinasi terbaik dari momentum dan RMSProp, sangat umum digunakan."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7c036ac",
      "metadata": {
        "id": "b7c036ac"
      },
      "source": [
        "### 3. Regularisasi untuk Menghindari Overfitting\n",
        "- **Early Stopping**: Hentikan pelatihan jika performa validasi menurun.\n",
        "- **Dropout**: Menonaktifkan neuron acak saat pelatihan.\n",
        "- **Regularisasi â„“1 dan â„“2**: Menambahkan penalti pada fungsi loss.\n",
        "- **Max-Norm**: Membatasi norma bobot maksimum per neuron."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4f24077",
      "metadata": {
        "id": "e4f24077"
      },
      "source": [
        "### 4. Tips dan Praktik Terbaik\n",
        "- **Gunakan callbacks** seperti `EarlyStopping` dan `ModelCheckpoint`.\n",
        "- **Visualisasi training** dengan TensorBoard.\n",
        "- **Coba batch normalization** di layer sebelum aktivasi.\n",
        "- **Gunakan optimizer seperti Adam** dengan learning rate default (0.001)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d1eef40",
      "metadata": {
        "id": "5d1eef40"
      },
      "source": [
        "### ðŸ“Œ Kesimpulan\n",
        "Bab ini memberikan strategi penting untuk melatih jaringan dalam secara efisien dan stabil. Menangani gradien, optimisasi, dan regularisasi adalah kunci keberhasilan deep learning."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}